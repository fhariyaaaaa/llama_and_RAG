# -*- coding: utf-8 -*-
"""TensorFlow_with_GPU (8).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OegFRje2MTjGD-7lVK74vwJHCRjK7ytN

# Tensorflow with GPU

This notebook provides an introduction to computing on a [GPU](https://cloud.google.com/gpu) in Colab. In this notebook you will connect to a GPU, and then run some basic TensorFlow operations on both the CPU and a GPU, observing the speedup provided by using the GPU.

## Enabling and testing the GPU

First, you'll need to enable GPUs for the notebook:

- Navigate to Editâ†’Notebook Settings
- select GPU from the Hardware Accelerator drop-down

Next, we'll confirm that we can connect to the GPU with tensorflow:
"""

import tensorflow as tf
device_name = tf.test.gpu_device_name()
if device_name != '/device:GPU:0':
  raise SystemError('GPU device not found')
print('Found GPU at: {}'.format(device_name))

pip install tensorflow transformers torch

!pip install pdfplumber

!pip install faiss-gpu

!pip install pdfplumber

import os
import pdfplumber
import re
from typing import List, Dict

# Step 1: Text Extraction from PDFs with Filtering
def extract_text_from_pdfs(pdf_folder: str = "/content") -> List[Dict]:
    """
    Extract text from all PDF files in the given folder, with additional filtering to remove
    lines containing only underscores or other non-informative repetitive patterns.

    Args:
        pdf_folder (str): Path to the folder containing PDF files. Default is "/content".

    Returns:
        List[Dict]: A list of dictionaries, each containing the file name, page number, and extracted text.
    """
    extracted_text = []
    for filename in os.listdir(pdf_folder):
        if filename.endswith(".pdf"):
            file_path = os.path.join(pdf_folder, filename)
            try:
                with pdfplumber.open(file_path) as pdf:
                    for page_num, page in enumerate(pdf.pages):
                        text = page.extract_text()
                        if text:
                            # Remove lines with only underscores or similar repetitive patterns
                            filtered_text = []
                            for line in text.splitlines():
                                if not re.match(r'^[\s_\-]+$', line):  # Skip lines with only underscores, dashes, or spaces
                                    filtered_text.append(line)
                            cleaned_text = "\n".join(filtered_text).strip()

                            if cleaned_text:
                                extracted_text.append({
                                    "file_name": filename,
                                    "page_number": page_num + 1,
                                    "text": cleaned_text
                                })
                            else:
                                print(f"Warning: No valid text extracted from file {filename}, page {page_num + 1}")
                        else:
                            print(f"Warning: No text extracted from file {filename}, page {page_num + 1}")
            except Exception as e:
                print(f"Error processing file {filename}: {e}")
    return extracted_text

# Execute text extraction
extracted_text = extract_text_from_pdfs()

# Display summary of extraction
print(f"\nTotal number of text entries extracted: {len(extracted_text)}")

# Count entries per file
entries_per_file = {}
for entry in extracted_text:
    file_name = entry['file_name']
    entries_per_file[file_name] = entries_per_file.get(file_name, 0) + 1

print("\nExtracted entries per file:")
for file_name, count in entries_per_file.items():
    print(f"- {file_name}: {count} entries")

# Display samples from the first 5 entries (or fewer if less than 5 were extracted)
print("\nSample extracted text:")
for entry in extracted_text[:5]:
    print(f"File: {entry['file_name']}")
    print(f"Page: {entry['page_number']}")
    print(f"Text sample: {entry['text'][:200]}...")  # First 200 characters
    print("\n---\n")

print("Extraction complete. You can now use 'extracted_text' for further processing.")

print(f"Number of documents extracted: {len(extracted_text)}")

import re
from typing import List, Dict

# Step 2: Chunking Text with Metadata

def chunk_text_with_metadata(extracted_text: List[Dict], chunk_size: int = 2000) -> List[Dict]:
    """
    Chunk the extracted text into larger, meaningful sections with metadata.

    Args:
        extracted_text (List[Dict]): A list of dictionaries containing extracted text with metadata.
        chunk_size (int): The maximum character length for each chunk.

    Returns:
        List[Dict]: A list of dictionaries, each containing the file name, page number, and chunked text.
    """
    chunks = []
    for entry in extracted_text:
        text = entry["text"]
        current_chunk = ""
        for line in text.split("\n"):
            if re.search(r'\b(APPEARANCES|COUNSEL PRESENT|INDEX|EXHIBIT|COURT|TRANSCRIPT|CIVIL DIVISION#|BEHALF OF|PAGE)\b', line, re.IGNORECASE):
                continue

            # Accumulate text til the chunk size is reached
            if len(current_chunk) + len(line) <= chunk_size:
                current_chunk += line + " "
            else:
                chunks.append({
                    "file_name": entry["file_name"],
                    "page_number": entry["page_number"],
                    "chunk_text": current_chunk.strip()
                })
                current_chunk = line + " "

        # Add the last chunk if it's not empty
        if current_chunk:
            chunks.append({
                "file_name": entry["file_name"],
                "page_number": entry["page_number"],
                "chunk_text": current_chunk.strip()
            })

    return chunks

# Execute chunking
chunks_with_metadata = chunk_text_with_metadata(extracted_text)

# Display summary of chunking
print(f"\nTotal number of chunks created: {len(chunks_with_metadata)}")

# Count chunks per file
chunks_per_file = {}
for chunk in chunks_with_metadata:
    file_name = chunk['file_name']
    chunks_per_file[file_name] = chunks_per_file.get(file_name, 0) + 1

print("\nChunks per file:")
for file_name, count in chunks_per_file.items():
    print(f"- {file_name}: {count} chunks")

# Display samples from the first 3 chunks (or fewer if less than 3 were created)
print("\nSample chunks:")
for chunk in chunks_with_metadata[:3]:
    print(f"File: {chunk['file_name']}")
    print(f"Page: {chunk['page_number']}")
    print(f"Chunk text sample: {chunk['chunk_text'][:200]}...")  # First 200 characters
    print("\n---\n")

print("Chunking complete. You can now use 'chunks_with_metadata' for further processing.")

import re
from typing import List, Dict
#cleaning
def clean_chunks(chunks: List[Dict]) -> List[Dict]:
    """
    Clean the text chunks by removing extra whitespace, short chunks, and applying additional cleaning steps.
    Retain the PDF metadata (file name and page number).

    Args:
        chunks (List[Dict]): A list of dictionaries containing chunked text with metadata.

    Returns:
        List[Dict]: A list of dictionaries with cleaned text and metadata.
    """
    cleaned_chunks = []
    for chunk in chunks:
        # Remove extra whitespace and normalize spaces
        cleaned_text = re.sub(r'\s+', ' ', chunk["chunk_text"]).strip()

        # Remove any remaining lines that are just page numbers or short headings
        cleaned_text = re.sub(r'^\d+$|^.{1,3}$', '', cleaned_text, flags=re.MULTILINE)

        # Remove common header/footer patterns
        cleaned_text = re.sub(r'Page \d+ of \d+|Confidential - Subject to Protective Order', '', cleaned_text)

        cleaned_text = re.sub(r'^[A-Z\s]+$', '', cleaned_text, flags=re.MULTILINE)

        # Final cleaning: remove any multiple newlines and trim
        cleaned_text = re.sub(r'\n+', '\n', cleaned_text).strip()


        if len(cleaned_text) > 100:
            cleaned_chunk = {
                "file_name": chunk["file_name"],
                "page_number": chunk["page_number"],
                "chunk_text": cleaned_text
            }
            cleaned_chunks.append(cleaned_chunk)

    return cleaned_chunks

# Execute cleaning
cleaned_chunks = clean_chunks(chunks_with_metadata)

# Display summary of cleaning
print(f"\nTotal number of chunks before cleaning: {len(chunks_with_metadata)}")
print(f"Total number of chunks after cleaning: {len(cleaned_chunks)}")

# Count cleaned chunks per file
cleaned_chunks_per_file = {}
for chunk in cleaned_chunks:
    file_name = chunk['file_name']
    cleaned_chunks_per_file[file_name] = cleaned_chunks_per_file.get(file_name, 0) + 1

print("\nCleaned chunks per file:")
for file_name, count in cleaned_chunks_per_file.items():
    print(f"- {file_name}: {count} chunks")

# Display samples from the first 3 cleaned chunks
print("\nSample cleaned chunks:")
for chunk in cleaned_chunks[:3]:
    print(f"File: {chunk['file_name']}")
    print(f"Page: {chunk['page_number']}")
    print(f"Cleaned chunk text sample: {chunk['chunk_text'][:200]}...")  # First 200 characters
    print("\n---\n")

print("Cleaning complete. You can now use 'cleaned_chunks' for further processing.")

pip install -U sentence-transformers

import faiss
import numpy as np
from sentence_transformers import SentenceTransformer
from typing import List, Dict, Tuple
import torch

#retrieval (RAG)
def index_documents(chunks: List[Dict], model_name: str = "all-MiniLM-L6-v2") -> Tuple[faiss.IndexFlatL2, np.ndarray, List[Dict]]:
    """
    Index the document chunks using FAISS-GPU for efficient retrieval.

    Args:
        chunks (List[Dict]): A list of dictionaries containing cleaned, chunked text with metadata.
        model_name (str): The name of the model used for generating embeddings.

    Returns:
        Tuple[faiss.IndexFlatL2, np.ndarray, List[Dict]]:
            - The FAISS index containing the document embeddings.
            - The numpy array of embeddings.
            - The list of chunks with their metadata.
    """
    # Load the Sentence Transformer model
    model = SentenceTransformer(model_name)
    model.to('cuda')

    # Generate embeddings for each chunk
    embeddings = []
    indexed_chunks = []
    for chunk in chunks:
        with torch.no_grad():
            embedding = model.encode(chunk["chunk_text"], convert_to_tensor=True).cpu().numpy()
        embeddings.append(embedding)
        indexed_chunks.append(chunk)

    # Convert embeddings to numpy array for FAISS indexing
    embeddings_np = np.array(embeddings).astype('float32')

    # Create a FAISS index and add the embeddings
    dimension = embeddings_np.shape[1]
    index = faiss.IndexFlatL2(dimension)

    # Move index to GPU
    res = faiss.StandardGpuResources()
    index = faiss.index_cpu_to_gpu(res, 0, index)

    index.add(embeddings_np)

    return index, embeddings_np, indexed_chunks

# Execute indexing
index, embeddings, indexed_chunks = index_documents(cleaned_chunks)

print("Document indexing completed.")
print(f"Number of indexed documents: {index.ntotal}")
print(f"Embedding dimension: {embeddings.shape[1]}")

# Count indexed chunks per file
indexed_chunks_per_file = {}
for chunk in indexed_chunks:
    file_name = chunk['file_name']
    indexed_chunks_per_file[file_name] = indexed_chunks_per_file.get(file_name, 0) + 1

print("\nIndexed chunks per file:")
for file_name, count in indexed_chunks_per_file.items():
    print(f"- {file_name}: {count} chunks")

def sample_search(query: str, index: faiss.IndexFlatL2, chunks: List[Dict], k: int = 5):
    model = SentenceTransformer("all-MiniLM-L6-v2")
    model.to('cuda')
    with torch.no_grad():
        query_vector = model.encode([query], convert_to_tensor=True).cpu().numpy()
    distances, indices = index.search(query_vector, k)

    print(f"\nTop {k} results for query: '{query}'\n")
    for i, idx in enumerate(indices[0]):
        print(f"Result {i+1}:")
        print(f"File: {chunks[idx]['file_name']}")
        print(f"Page: {chunks[idx]['page_number']}")
        print(f"Text: {chunks[idx]['chunk_text'][:200]}...")  # First 200 characters
        print(f"Distance: {distances[0][i]}")
        print("---")

# Perform a sample search
sample_query = "What are the health hazards of asbestos?"
sample_search(sample_query, index, indexed_chunks)

print("\nIndexing and sample search complete.")

import torch
from sentence_transformers import SentenceTransformer
from typing import List, Dict
import faiss

def retrieve_relevant_chunks(query: str, index: faiss.IndexFlatL2, chunks: List[Dict], model_name: str = "all-MiniLM-L6-v2", top_k: int = 5) -> List[Dict]:
    """
    Retrieve the most relevant document chunks for a given query using FAISS.

    Args:
        query (str): The query for which to retrieve relevant document chunks.
        index (faiss.IndexFlatL2): The FAISS index containing document embeddings.
        chunks (List[Dict]): A list of dictionaries containing cleaned, chunked text with metadata.
        model_name (str): The name of the model used for generating embeddings.
        top_k (int): The number of top relevant chunks to retrieve.

    Returns:
        List[Dict]: A list of dictionaries containing the most relevant chunks for the query.
    """
    # Load the Sentence Transformer model
    model = SentenceTransformer(model_name)
    model.to('cuda')  # Move model to GPU

    # Generate the embedding for the query
    with torch.no_grad():
        query_embedding = model.encode(query, convert_to_tensor=True).cpu().numpy().reshape(1, -1).astype('float32')

    # Search the FAISS index for the most similar embeddings
    distances, indices = index.search(query_embedding, top_k)

    # Retrieve the most relevant chunks using the indices returned by FAISS
    relevant_chunks = [
        {
            "chunk": chunks[idx],
            "distance": distances[0][i]
        }
        for i, idx in enumerate(indices[0]) if idx != -1
    ]

    return relevant_chunks

def process_query(query: str, index: faiss.IndexFlatL2, chunks: List[Dict], top_k: int = 5):
    """
    Process a query and display relevant chunks with their metadata.

    Args:
        query (str): The query to process.
        index (faiss.IndexFlatL2): The FAISS index containing document embeddings.
        chunks (List[Dict]): A list of dictionaries containing cleaned, chunked text with metadata.
        top_k (int): The number of top relevant chunks to retrieve.
    """
    relevant_chunks = retrieve_relevant_chunks(query, index, chunks, top_k=top_k)

    print(f"\nQuery: {query}")
    print(f"\nTop {len(relevant_chunks)} Relevant Chunks:\n")
    for i, result in enumerate(relevant_chunks, 1):
        chunk = result["chunk"]
        print(f"Result {i}:")
        print(f"File: {chunk['file_name']}")
        print(f"Page: {chunk['page_number']}")
        print(f"Text: {chunk['chunk_text'][:300]}...")  # First 300 characters
        print(f"Distance: {result['distance']}")
        print("---")
#Augumentation (RAG)
# Example usage
sample_queries = [
    "What are the health hazards of asbestos?",
    "How is asbestos exposure related to increased risk?",
    "What protective measures were recommended when working with asbestos?",
    "What are the legal implications of asbestos use in products?",
    "How has the understanding of asbestos dangers evolved over time?"
]

print("Processing sample queries...")
for query in sample_queries:
    process_query(query, index, indexed_chunks)

print("\nYou can now enter your own queries. Type 'exit' to end.")
while True:
    user_query = input("\nEnter your query: ")
    if user_query.lower() == 'exit':
        break
    process_query(user_query, index, indexed_chunks)

print("\nQuery processing complete.")

from huggingface_hub import login

login(token="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")

from transformers import AutoTokenizer, LlamaForCausalLM

model_name = "meta-llama/Llama-3.1-8B-Instruct"

# Load the tokenizer and model using AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = LlamaForCausalLM.from_pretrained(model_name, device_map="auto")

!pip install sentencepiece

!pip install torch

tokenizer.pad_token = tokenizer.eos_token

input_text = "What are the health hazards of asbestos?"

# Tokenize with attention mask and padding
inputs = tokenizer(
    input_text,
    return_tensors="pt",
    padding=True,
    truncation=True
).to(model.device)

# Generate text
output = model.generate(
    **inputs,
    max_length=450,  #len of output
    num_return_sequences=1,  # to generate a single output
    do_sample=True,
    num_beams=3,
    no_repeat_ngram_size=2,
    early_stopping=True,
    temperature=0.7,
    pad_token_id=tokenizer.eos_token_id
)

#GENERATION OF RAG
# Decode the output tokens to text
output_text = tokenizer.decode(output[0], skip_special_tokens=True)
print(output_text)

"""
# **FAISS Retrieval + Context Augmentation + Llama 3.1 Generation = RAG.**"""

!pip install nltk rouge-score

import nltk
from nltk.translate.bleu_score import sentence_bleu
from rouge_score import rouge_scorer

# Make sure to download the necessary NLTK data
nltk.download('punkt')

# Example queries and reference responses
queries = [
    "What are the health hazards of asbestos?",
    "When did Pneumo Abex start making asbestos-containing brakes?",
    "When did Pneumo Abex first become aware of the hazards of asbestos?"
]

reference_responses = [
    "Asbestos exposure can cause mesothelioma, lung cancer, and asbestosis.",
    "Pneumo Abex started making asbestos-containing brakes in the 1920s.",
    "Pneumo Abex first became aware of the hazards of asbestos in the early 1960s."
]
generated_responses = [
    "Asbestos exposure can lead to serious health problems like mesothelioma, lung cancer, and asbestosis.",
    "Pneumo Abex started manufacturing asbestos-containing brakes in the 1920s.",
    "Pneumo Abex first became aware of the hazards of asbestos in the early 1960s."
]

for ref, gen in zip(reference_responses, generated_responses):
    reference = [ref.split()]  # Tokenize reference response
    candidate = gen.split()  # Tokenize generated response
    bleu_score = sentence_bleu(reference, candidate)
    print(f"BLEU Score: {bleu_score:.2f}\nReference: {ref}\nGenerated: {gen}\n")

# Create a ROUGE scorer
scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)

for ref, gen in zip(reference_responses, generated_responses):
    scores = scorer.score(ref, gen)
    print(f"ROUGE Scores:\nROUGE-1: {scores['rouge1'].fmeasure:.2f}\nROUGE-2: {scores['rouge2'].fmeasure:.2f}\nROUGE-L: {scores['rougeL'].fmeasure:.2f}\n")
    print(f"Reference: {ref}\nGenerated: {gen}\n")

pip install gradio

import gradio as gr
import torch


def answer_query(query):
    try:
        input_prompt = f"Answer the following question concisely: {query}"
        inputs = tokenizer(input_prompt, return_tensors="pt").to(model.device)

        # Generate a response
        with torch.no_grad():
            output = model.generate(
                **inputs,
                max_length=150,  # Limit max length to reduce response time
                num_return_sequences=1,
                no_repeat_ngram_size=2,
                early_stopping=True,
                temperature=0.5,
                top_p=0.8,
                num_beams=1
            )

        # Decode the generated response
        response = tokenizer.decode(output[0], skip_special_tokens=True)


        response = response.replace(input_prompt, "").strip()

        if query in response:
            response = response.replace(query, "").strip()

        # Check if response ends abruptly
        if response.endswith(('.', '!', '?')) is False:
            response += " (The answer may continue, please ask for more details if needed.)"

        return response

    except RuntimeError as e:
        # Catch CUDA out-of-memory errors
        if "out of memory" in str(e):
            torch.cuda.empty_cache()
            return "Error: Out of memory. Try simplifying the question or reducing the context length."
        else:
            return f"Error: {str(e)}"

# Define the Gradio interface
interface = gr.Interface(
    fn=answer_query,
    inputs="text",
    outputs="text",
    title="Legal Document Question Answering",
    description="Hey Lawyer! Ask questions about the legal documents.",
)

# Using vertical block layout
with gr.Blocks() as demo:
    gr.Markdown("# Legal Document Question Answering")
    gr.Markdown("### Hey Lawyer! Ask questions about the legal documents below.")

    with gr.Column():
        query_input = gr.Textbox(label="Your Question:")
        submit_btn = gr.Button("Submit", variant="primary")
        clear_btn = gr.Button("Clear", variant="secondary")
        response_output = gr.Textbox(label="Answer:", interactive=False)

    submit_btn.click(answer_query, inputs=query_input, outputs=response_output)
    clear_btn.click(lambda: ("", ""), inputs=None, outputs=[query_input, response_output])

# Launch the Gradio app
demo.launch(share=True)

gradio_script = """
import gradio as gr
import torch


def answer_query(query):
    try:
        input_prompt = f"Answer the following question concisely: {query}"
        inputs = tokenizer(input_prompt, return_tensors="pt").to(model.device)

        # Generate a response
        with torch.no_grad():
            output = model.generate(
                **inputs,
                max_length=150,  # Limit max length to reduce response time
                num_return_sequences=1,
                no_repeat_ngram_size=2,
                early_stopping=True,
                temperature=0.5,
                top_p=0.8,
                num_beams=1
            )

        # Decode the generated response
        response = tokenizer.decode(output[0], skip_special_tokens=True)


        response = response.replace(input_prompt, "").strip()

        if query in response:
            response = response.replace(query, "").strip()

        # Check if response ends abruptly
        if response.endswith(('.', '!', '?')) is False:
            response += " (The answer may continue, please ask for more details if needed.)"

        return response

    except RuntimeError as e:
        # Catch CUDA out-of-memory errors
        if "out of memory" in str(e):
            torch.cuda.empty_cache()
            return "Error: Out of memory. Try simplifying the question or reducing the context length."
        else:
            return f"Error: {str(e)}"

# Define the Gradio interface
interface = gr.Interface(
    fn=answer_query,
    inputs="text",
    outputs="text",
    title="Legal Document Question Answering",
    description="Hey Lawyer! Ask questions about the legal documents.",
)

# Using vertical block layout
with gr.Blocks() as demo:
    gr.Markdown("# Legal Document Question Answering")
    gr.Markdown("### Hey Lawyer! Ask questions about the legal documents below.")

    with gr.Column():
        query_input = gr.Textbox(label="Your Question:")
        submit_btn = gr.Button("Submit", variant="primary")
        clear_btn = gr.Button("Clear", variant="secondary")
        response_output = gr.Textbox(label="Answer:", interactive=False)

    submit_btn.click(answer_query, inputs=query_input, outputs=response_output)
    clear_btn.click(lambda: ("", ""), inputs=None, outputs=[query_input, response_output])

# Launch the Gradio app
demo.launch(share=True)
"""

with open('gradio_app.py', 'w') as f:
    f.write(gradio_script)



